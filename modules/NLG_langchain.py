"""è¯¥æ–‡ä»¶å®šä¹‰äº†èŠå¤©æœºå™¨äººçš„åç«¯ç±»"""
# https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo
import _thread as thread
import hashlib
import hmac
import json
import re
from abc import abstractmethod
from base64 import b64encode
from random import randint
from ssl import CERT_NONE as SSL_CERT_NONE
from urllib.parse import urljoin, urlparse, urlencode
from typing import List
import google.api_core.exceptions
import requests
from websocket import WebSocketApp
from langchain.chat_models import AzureChatOpenAI
from langchain.chat_models import ChatOpenAI
from langchain.embeddings import OpenAIEmbeddings, HuggingFaceEmbeddings

from langchain.callbacks.base import BaseCallbackHandler

from langchain.vectorstores import FAISS
from langchain.chains import ConversationalRetrievalChain
from langchain.chains.conversational_retrieval.base import BaseConversationalRetrievalChain
from langchain.prompts import PromptTemplate

from langchain.document_loaders import (UnstructuredPowerPointLoader, UnstructuredWordDocumentLoader, PyPDFLoader, UnstructuredFileLoader, CSVLoader, MWDumpLoader)

from langchain.text_splitter import (RecursiveCharacterTextSplitter, CharacterTextSplitter)
from modules.utils import NLGEnum, Configs, Message  # getRFC1123, getMacAddress
import os




class NLGBase:
    """èŠå¤©æœºå™¨äººåŸºç±»ï¼Œå»ºè®®åœ¨è¿›è¡ŒèŠå¤©æœºå™¨äººå¼€å‘æ—¶ç»§æ‰¿è¯¥ç±»"""
    try:
        import tiktoken
        tokenEncoding = tiktoken.get_encoding("cl100k_base")
    except ImportError:
        tokenEncoding = None

    def __init__(self, nlg_type: NLGEnum, model: str, prompt: str = None):
        self.type = nlg_type  # æœºå™¨äººç±»å‹
        self.model = model  # æœºå™¨äººæ¨¡å‹
        self.prompt = prompt  # é»˜è®¤æç¤ºè¯­(ç”¨äºæŒ‡å®šæœºå™¨äººçš„èº«ä»½ï¼Œæœ‰åŠ©äºæé«˜é’ˆå¯¹ç‰¹å®šé¢†åŸŸé—®é¢˜çš„æ•ˆæœ)ï¼Œä¼˜å…ˆçº§ä½äºæŸ¥è¯¢æ—¶ä¼ å…¥çš„prompt


    @abstractmethod
    def singleQuery(self, message: str, prompt: str = None) -> str:
        """
        ç®€å•åœ°è¿›è¡Œå•æ¬¡æŸ¥è¯¢

        å…è®¸ä¸ºæœ¬æ¬¡æŸ¥è¯¢å•ç‹¬æŒ‡å®špromptï¼Œä½¿ç”¨ä¼˜å…ˆçº§æŒ‰ç…§ å‚æ•°prompt > å…¨å±€prompt > None çš„é¡ºåº
        :param message: str æœ¬æ¬¡ç”¨æˆ·è¾“å…¥
        :param prompt: str æç¤ºè¯­(ç”¨äºæŒ‡å®šæœºå™¨äººçš„èº«ä»½ï¼Œæœ‰åŠ©äºæé«˜é’ˆå¯¹ç‰¹å®šé¢†åŸŸé—®é¢˜çš„æ•ˆæœ)
        :return: str å¯¹æœ¬æ¬¡èŠå¤©çš„å›å¤å†…å®¹
        """

    @abstractmethod
    def continuedQuery(self, message: str, history: list[list[str, str]], prompt: str = None):
        """
        è¿›è¡Œå¸¦æœ‰å†å²è®°å½•çš„æŸ¥è¯¢

        æŒ‰ç…§gradioå®˜æ–¹demo chatbot_simpleçš„å¼€å‘ç¤ºä¾‹ï¼Œè‹¥æ˜¯æƒ³å®ç°å¸¦æœ‰å†å²è®°å½•çš„å¯¹è¯ï¼Œæ— éœ€è‡ªè¡Œå®ç°å†å²è®°å½•çš„ä¿ç•™ï¼Œå»ºè®®çš„åšæ³•æ˜¯åœ¨gradioä¸­æ›´æ–°å†å²è®°å½•ï¼Œæ¯æ¬¡æŸ¥è¯¢æ—¶åªéœ€è¦è°ƒç”¨æ›´æ–°åçš„historyå³å¯ã€‚

        å¤§å¤šæ•°APIè¦æ±‚å†å²è®°å½•(å«prompt)çš„æ ¼å¼åº”å½“ä¸º[{"role": "user", "content": ""}, {"role": "assistant", "message":
        ""}...]ï¼Œè€Œå¤§å¤šæ•°å‰ç«¯çš„å†å²è®°å½•æ ¼å¼ä¸º[[str, str]...]ï¼Œå› æ­¤éœ€è¦åœ¨è°ƒç”¨å‰è¿›è¡Œè½¬æ¢ã€‚å¯å€ŸåŠ©historyConverteræ–¹æ³•è¿›è¡Œè½¬æ¢ã€‚
        :param message: str æœ¬æ¬¡ç”¨æˆ·è¾“å…¥
        :param history: List[List[str, str]...] åˆ†åˆ«ä¸ºç”¨æˆ·è¾“å…¥å’Œæœºå™¨äººå›å¤(å…ˆå‰çš„)
        :param prompt: str æç¤ºè¯­(ç”¨äºæŒ‡å®šæœºå™¨äººçš„èº«ä»½ï¼Œæœ‰åŠ©äºæé«˜é’ˆå¯¹ç‰¹å®šé¢†åŸŸé—®é¢˜çš„æ•ˆæœ)
        """

    @abstractmethod
    def checkConnection(self):
        """
        æ£€æŸ¥ä¸Hostçš„è¿æ¥çŠ¶æ€
        å¯¹äºå¤šæ•°æœªè®¾è®¡æ£€æŸ¥è¿æ¥çŠ¶æ€çš„APIï¼Œå¯å‚è€ƒOpenAIçš„åšæ³•ï¼šç›´æ¥è®©åç«¯å›å¤ä¸€å¥ç®€å•çš„è¯ï¼Œè‹¥å›å¤æˆåŠŸåˆ™è‡ªç„¶è¿æ¥æˆåŠŸã€‚
        """

    def converterHistory(self, history: [[str, str]], prompt: str = None) -> list[Message]:
        """
        å°†[[str, str]...]å½¢å¼çš„å†å²è®°å½•è½¬æ¢ä¸º[{"role": "user", "content": ""}, {"role": "assistant", "content": ""}...]çš„æ ¼å¼ï¼Œ
        ä½¿ç”¨åœºæ™¯æ˜¯å°†gradioçš„ChatbotèŠå¤©è®°å½•æ ¼å¼è½¬æ¢ä¸ºChatGPT/ChatGLM3çš„èŠå¤©è®°å½•æ ¼å¼
        :param history: [[str, str]...] åˆ†åˆ«ä¸ºç”¨æˆ·è¾“å…¥å’Œæœºå™¨äººå›å¤(å…ˆå‰çš„)
        :param prompt: str æç¤ºè¯­(ç”¨äºæŒ‡å®šæœºå™¨äººçš„èº«ä»½ï¼Œæœ‰åŠ©äºæé«˜é’ˆå¯¹ç‰¹å®šé¢†åŸŸé—®é¢˜çš„æ•ˆæœï¼Œå…è®¸ä¸ºç©º)
        :return: [{"role": "user", "content": ""}, {"role": "assistant", "content": ""}...]çš„æ ¼å¼çš„å†å²è®°å½•ï¼Œæ³¨æ„ï¼Œè¯¥ç»“æœä¸åŒ…æ‹¬
        æœ¬æ¬¡çš„ç”¨æˆ·è¾“å…¥ï¼Œä»…è½¬æ¢äº†å†å²è®°å½•
        """
        sessionPrompt = prompt if prompt else self.prompt
        sessionHistory = [Message(role="system", content=sessionPrompt)] if sessionPrompt else []
        for chat in history:
            sessionHistory.append(Message(role="user", content=chat[0]))
            sessionHistory.append(Message(role="assistant", content=chat[1]))
        return sessionHistory

    @staticmethod
    def lenOfMessages(history: list[Message] = None, message: str = None) -> int:
        """
        è®¡ç®—å†å²è®°å½•ä¸­ï¼Œcontentéƒ¨åˆ†çš„æ€»é•¿åº¦ï¼Œä»¥ä¾¿äºåˆ¤æ–­æ˜¯å¦è¶…å‡ºäº†APIçš„tokenæ•°é™åˆ¶ã€‚

        è‹¥å·²å°†æœ¬æ¬¡è¾“å…¥è®¡å…¥å†å²è®°å½•ï¼Œåˆ™æ— éœ€ä¼ å…¥message

        æˆ–è€…å¯ä»¥åªä¼ å…¥messageï¼Œä¸ä¼ å…¥historyï¼Œä»…ç»Ÿè®¡æœ¬æ¬¡è¾“å…¥çš„é•¿åº¦
        :param history: list[Message] å†å²è®°å½•
        :param message: str æœ¬æ¬¡ç”¨æˆ·è¾“å…¥
        """
        length = 0
        if history:
            length += sum([len(message["content"]) for message in history])
        if message:
            length += len(message)
        return length

    @staticmethod
    def lenOfTokens(history: list[Message] = None, message: str = None, token_per_zh_char: float = None) -> int:
        """
        ä¼°ç®—æ¶ˆæ¯çš„tokené•¿åº¦
        :param history: list[Message] å†å²è®°å½•
        :param message: str æœ¬æ¬¡ç”¨æˆ·è¾“å…¥
        :param token_per_zh_char: float æ¯ä¸ªä¸­æ–‡å­—ç¬¦çš„tokenæ•°ï¼Œæ ¹æ®ç»éªŒï¼Œå•ä¸ªæ±‰å­—çš„tokenæ•°å¤§æ¦‚ä¸º1/2~4/3ã€‚åœ¨ä½¿ç”¨tiktokenåº“æ—¶ï¼Œè¯¥å‚æ•°æ— æ•ˆ
        :return: int ä¼°ç®—çš„tokenæ•°ã€‚ç”±äºä¸åŒçš„APIçš„tokenizeæ–¹æ³•ä¸åŒï¼Œå› æ­¤è¯¥ç»“æœä»…ä¾›å‚è€ƒ
        """
        content = ""
        if history:
            for message in history:
                content += message["content"]
        if message:
            content += message
        if NLGBase.tokenEncoding:
            return len(NLGBase.tokenEncoding.encode(content))
        else:  # æœªå®‰è£…tiktokenåº“ï¼Œåˆ™åªèƒ½æ ¹æ®ç»éªŒè¿›è¡Œä¼°ç®—
            # æ ¹æ® Moonshot AIçš„æ–‡æ¡£(https://platform.moonshot.cn/docs/docs#åŸºæœ¬æ¦‚å¿µä»‹ç»)ä¸­çš„ç»éªŒä»‹ç»ï¼Œ
            # tokenå’Œæ±‰å­—çš„æ¯”ä¾‹å¤§çº¦ä¸º1:1.5~1:2ï¼Œè€Œtokenå’Œè‹±æ–‡å•è¯çš„æ¯”ä¾‹å¤§çº¦ä¸º1:1.2ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥æ ¹æ®è¿™ä¸ªæ¯”ä¾‹è¿›è¡Œä¼°ç®—
            token_per_en_word = 1.2  # è‹±æ–‡å•è¯çš„tokenæ•°
            if not token_per_zh_char:
                token_per_zh_char = 1.33  # å–1:1.5
            word_list = re.split("[ ,.?!';:()\[\]{}\t\n]", content)  # åˆ†è¯
            word_list = [word for word in word_list if word != ""]  # å»é™¤ç©ºå­—ç¬¦ä¸²
            token_num = 0
            for word in word_list:
                if word.isascii():
                    token_num += token_per_en_word
                else:  # æ­¤æ—¶ä¸ºè‹±æ–‡ä¸å…¶ä»–è¯­è¨€æ··åˆçš„æƒ…å†µ
                    sub_word_list = re.split("[\u4e00-\u9fff]", word)  # æ ¹æ®ä¸­æ–‡å­—ç¬¦åˆ†è¯
                    zh_char_num = sub_word_list.count("")  # ç©ºå­—ç¬¦ä¸²çš„æ•°é‡å³ä¸ºä¸­æ–‡å­—ç¬¦çš„æ•°é‡
                    token_num += zh_char_num * token_per_zh_char
                    token_num += (len(sub_word_list) - zh_char_num) * token_per_en_word  # è‹±æ–‡å•è¯çš„æ•°é‡
            return int(token_num)


class StreamHandler(BaseCallbackHandler):
    def __init__(self, container, initial_text=""):
        self.container = container
        self.text = initial_text

    def on_llm_new_token(self, token: str, **kwargs) -> None:
        self.text += token
        self.container.markdown(self.text)


class ChatGPT(NLGBase):
    """
    é€šè¿‡APIè°ƒç”¨ChatGPTè¿›è¡Œé—®ç­”

    APIæ–‡æ¡£å‚è€ƒï¼šhttps://chatanywhere.apifox.cn/
    """

    def __init__(self, OpenAI_config: dict, prompt: str = None, answer_container=None, condense_question_container=None):
        super().__init__(NLGEnum.ChatGPT, OpenAI_config.get("gpt_model", "gpt-3.5-turbo"), prompt)
        self.api_key = OpenAI_config.get("api_key", None)
        self.temperature = OpenAI_config.get("temperature", 0)
        self.request_timeout = OpenAI_config.get("request_timeout", 30)
        self.streaming = OpenAI_config.get("streaming", False)
        self.answer_container = answer_container
        self.condense_question_container = condense_question_container
        self.chunk_size = OpenAI_config.get("chunk_size", 10000)
        self.chunk_overlap = OpenAI_config.get("chunk_overlap", 0)
        self.text_splitter = RecursiveCharacterTextSplitter(chunk_size=self.chunk_size, chunk_overlap=self.chunk_overlap)
        self.max_tokens = OpenAI_config.get("max_tokens", 20000)
        
        if not self.api_key:
            raise ValueError("OpenAI api_key is not set! Please check your 'config.json' file.")
        
        self.chatgpt = ChatOpenAI(
                # max_tokens = self.max_tokens,
                temperature=self.temperature,
                openai_api_key=self.api_key,
                request_timeout=self.request_timeout,
                model=self.model,  # Model name is needed for openai.com only
                streaming=self.streaming,
                callbacks=[StreamHandler(answer_container)] if self.streaming else []
            ) # type: ignore
        
        if self.streaming:
            self.condense_question_llm = ChatOpenAI(
                temperature=self.temperature,
                openai_api_key=self.api_key,
                request_timeout=self.request_timeout,
                model=self.model,  # Model name is needed for openai.com only
                streaming=True,
                callbacks=[StreamHandler(self.condense_question_container, "ğŸ¤”...")]
            ) # type: ignore
        else:
            self.condense_question_llm = self.chatgpt

        # embeddings = OpenAIEmbeddings(
        #         deployment='"text-embedding-ada-002"',
        #         chunk_size=1
        #         ) # type: ignore
        self.embeddings = HuggingFaceEmbeddings(
            model_name=(
                'sentence-transformers/'
                'multi-qa-MiniLM-L6-cos-v1'
            )
        )

        self.checkConnection()

    

    def init_chatchain(self, chain_type : str = "stuff") -> None:
        # stuff chain_type seems working better than others
        CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template("""è¯„ä»·æ ‡å‡†:{prompt}
            Chat History:
            {chat_history}

            Follow Up Input:
            {question}

            Standalone Question:"""
            )            
        self.chatchain = ConversationalRetrievalChain.from_llm(llm=self.chatgpt, 
                                                retriever=self.vector_db.as_retriever(search_type="similarity", search_kwargs={"k":4}),
                                                condense_question_prompt=CONDENSE_QUESTION_PROMPT,
                                                condense_question_llm=self.condense_question_llm,
                                                chain_type=chain_type,
                                                return_source_documents=True,
                                                verbose=True)
                                                # combine_docs_chain_kwargs=dict(return_map_steps=False))

    def init_vector_db_from_documents(self, file_list: List[str]=[]):
        docs = []
        for file in file_list:
            print(f"Loading file: {file}")
            ext_name = os.path.splitext(file)[-1]
            # print(ext_name)

            if ext_name == ".pptx":
                loader = UnstructuredPowerPointLoader(file)
            elif ext_name == ".docx":
                loader = UnstructuredWordDocumentLoader(file)
            elif ext_name == ".pdf":
                loader = PyPDFLoader(file)
            elif ext_name == ".csv":
                loader = CSVLoader(file_path=file)
            elif ext_name == ".xml":
                loader = MWDumpLoader(file_path=file, encoding="utf8")
            else:
                # process .txt, .html
                loader = UnstructuredFileLoader(file)

            doc = loader.load_and_split(self.text_splitter)            
            docs.extend(doc)
            print("Processed document: " + file)

        print("Generating embeddings and ingesting to vector db.")
        self.vector_db = FAISS.from_documents(docs, self.embeddings)
        print("Vector db initialized.")
        return {'message':'Vector db initialized.'}

        # print("Vector db initialized.")
        # FAISS.save_local(self.vector_db, 'path', 'index_name')
        # print("Vector db saved to local")

    


    def continuedQuery(self, message: str, history: list[list[str, str]], prompt: str = None):
        
        session_history = []
        for chat in history:
            q = "" if chat[0] == None else chat[0]
            a = "" if chat[1] == None else chat[1]
            # remove details for reference to reduce token
            a = re.sub(r"<details>.*</details>", "", a)
            session_history.append((q, a))       
        response = self.get_chatgpt_result(messages=message, chat_history=session_history, prompt=prompt)
        return response




    def get_chatgpt_result(self, prompt,messages, chat_history):
        result = self.chatchain({
                "prompt": prompt, 
                "question": messages,
                "chat_history": chat_history
        },
        return_only_outputs=True)
        return result['answer']


    def checkConnection(self):
        """
        æ£€æŸ¥ä¸OpenAIçš„è¿æ¥çŠ¶æ€(é€šè¿‡ä¸€æ¬¡ç®€å•çš„é—®ç­”ï¼Œä»¥æµ‹è¯•APIå¯ç”¨æ€§)
        :return: bool æ˜¯å¦è¿æ¥æˆåŠŸ
        """
        try:
            response = requests.post(
                url="https://api.openai.com/v1/chat/completions",
                headers={
                    "Content-Type": "application/json",
                    "Authorization": f"Bearer {self.api_key}"
                },
                json={
                    "model": "gpt-3.5-turbo",
                    "messages": [Message(role="user", content="Say this is a test!")]
                },
            )
            if response.status_code != 200:
                raise ConnectionError(f"Connect to {self.model} failed, please check your network and API status.")
            else:
                print(f"Connected to {self.model} successfully.")
        except requests.exceptions.Timeout:
            raise TimeoutError(f"Connection to {self.model} timed out, please check your network status.")
        except requests.exceptions.ConnectionError:
            raise ConnectionError(f"Connect to {self.model} failed, please check your network and API status.")
        except ConnectionError as e:
            raise e
        finally:
            print("OpenAI connection check finished.")